{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5673c1",
   "metadata": {},
   "source": [
    "# Prefix-Tuning Reproduction \n",
    "This notebook reproduces the results from the ACL 2021 paper \"Prefix-Tuning: Optimizing Continuous Prompts for Generation\" and extends it with a sentiment-controlled prefix tuning example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf1a43",
   "metadata": {},
   "source": [
    "## Reproduction\n",
    "I validate the prefix-tuning method on a small text dataset to verify generation capabilities on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece22568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from evaluate import load\n",
    "from evaluate import load\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e81dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/90 [00:14<01:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2387, 'grad_norm': 23.64571762084961, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/90 [00:27<01:41,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6231, 'grad_norm': 22.093475341796875, 'learning_rate': 3.888888888888889e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30/90 [00:41<01:22,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5956, 'grad_norm': 13.479415893554688, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 40/90 [00:57<01:18,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2707, 'grad_norm': 19.204885482788086, 'learning_rate': 2.777777777777778e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50/90 [01:13<01:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1147, 'grad_norm': 18.654693603515625, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 60/90 [01:28<00:49,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1731, 'grad_norm': 20.207866668701172, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70/90 [01:46<00:34,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0669, 'grad_norm': 18.6934871673584, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 80/90 [02:04<00:18,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8665, 'grad_norm': 17.730138778686523, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [02:19<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8804, 'grad_norm': 13.691146850585938, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'train_runtime': 139.6458, 'train_samples_per_second': 1.289, 'train_steps_per_second': 0.644, 'train_loss': 1.536633268992106, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=90, training_loss=1.536633268992106, metrics={'train_runtime': 139.6458, 'train_samples_per_second': 1.289, 'train_steps_per_second': 0.644, 'total_flos': 0.0, 'train_loss': 1.536633268992106, 'epoch': 3.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Sentiment Prefix Model ===\n",
    "class SentimentPrefixModel(torch.nn.Module):\n",
    "    def __init__(self, base_model, prefix_len=10, hidden_size=768, use_mlp=True):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.prefix_len = prefix_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.prefix_embeddings = torch.nn.Embedding(2, prefix_len * hidden_size)\n",
    "        self.use_mlp = use_mlp\n",
    "\n",
    "        if self.use_mlp:\n",
    "            self.mlp = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size, hidden_size),\n",
    "                torch.nn.Tanh(),\n",
    "                torch.nn.Linear(hidden_size, hidden_size)\n",
    "            )\n",
    "\n",
    "    def forward(self, input_ids, sentiment_id, labels=None):\n",
    "        prefix_embed = self.prefix_embeddings(sentiment_id).view(-1, self.prefix_len, self.hidden_size)\n",
    "        if self.use_mlp:\n",
    "            prefix_embed = self.mlp(prefix_embed)\n",
    "\n",
    "        input_embed = self.base_model.transformer.wte(input_ids)\n",
    "        input_with_prefix = torch.cat((prefix_embed, input_embed), dim=1)\n",
    "\n",
    "        if labels is not None:\n",
    "            pad = torch.full((labels.size(0), self.prefix_len), -100).to(labels.device)\n",
    "            padded_labels = torch.cat([pad, labels], dim=1)\n",
    "        else:\n",
    "            padded_labels = None\n",
    "\n",
    "        return self.base_model(inputs_embeds=input_with_prefix, labels=padded_labels)\n",
    "\n",
    "# === Tokenizer & Data Loading ===\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "with open(\"src/smiles_sentiment_dataset_explicit.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sentiments = {\"positive\": 0, \"negative\": 1}\n",
    "inputs = []\n",
    "for item in data:\n",
    "    tokens = tokenizer(item[\"text\"], return_tensors=\"pt\", padding=\"max_length\", max_length=32, truncation=True)\n",
    "    tokens[\"sentiment_id\"] = torch.tensor([sentiments[item[\"prefix\"]]])\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
    "    inputs.append(tokens)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, items): self.items = items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i): return {k: v.squeeze(0) for k, v in self.items[i].items()}\n",
    "\n",
    "# === Prefix Initialization ===\n",
    "def initialize_prefix_embeddings_smart(prefix_embeddings, tokenizer, base_model, prefix_len):\n",
    "    init_words = [\"positive\", \"negative\"]\n",
    "    for i, word in enumerate(init_words):\n",
    "        tokens = tokenizer(word, return_tensors=\"pt\").input_ids\n",
    "        with torch.no_grad():\n",
    "            embed = base_model.transformer.wte(tokens).mean(dim=1)\n",
    "        prefix_embeddings.weight.data[i] = embed.repeat(1, prefix_len).view(-1)\n",
    "\n",
    "# === Load Model & Train ===\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model = SentimentPrefixModel(base_model)\n",
    "initialize_prefix_embeddings_smart(model.prefix_embeddings, tokenizer, base_model, model.prefix_len)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output/sentiment_prefix\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=MyDataset(inputs),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sentiment=\"positive\", prompt=\"I started my day and\", max_length=40):\n",
    "    sid = torch.tensor([0 if sentiment == \"positive\" else 1])\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "    prefix = model.prefix_embeddings(sid).view(1, model.prefix_len, model.hidden_size)\n",
    "    if model.use_mlp:\n",
    "        prefix = model.mlp(prefix)\n",
    "\n",
    "    input_embed = model.base_model.transformer.wte(input_ids)\n",
    "    input_with_prefix = torch.cat((prefix, input_embed), dim=1)\n",
    "\n",
    "    prefix_mask = torch.ones((1, model.prefix_len), dtype=torch.long)\n",
    "    full_attention_mask = torch.cat((prefix_mask, attention_mask), dim=1)\n",
    "\n",
    "    output = model.base_model.generate(\n",
    "        inputs_embeds=input_with_prefix,\n",
    "        attention_mask=full_attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d5b24",
   "metadata": {},
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "21d2daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a joy to be a part of.\n"
     ]
    }
   ],
   "source": [
    "generate(\"positive\", \"For me SMILES was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "92c5dc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " just a buzzy-tastes-free experience.\n"
     ]
    }
   ],
   "source": [
    "generate(\"negative\", \"For me SMILES was\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b3bc6",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a9eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# === Define Evaluation Function ===\n",
    "def evaluate_model(model, tokenizer, test_data, prefix_len=10, use_mlp=True, max_gen_length=40, num_samples=100):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for item in test_data[:num_samples]:\n",
    "        sentiment = item[\"prefix\"]\n",
    "        prompt = item.get(\"prompt\", item[\"text\"][:30]) \n",
    "        reference = item[\"text\"]\n",
    "\n",
    "        sid = torch.tensor([0 if sentiment == \"positive\" else 1])\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask\n",
    "\n",
    "        # Get prefix embeddings\n",
    "        prefix = model.prefix_embeddings(sid).view(1, prefix_len, model.hidden_size)\n",
    "        if use_mlp:\n",
    "            prefix = model.mlp(prefix)\n",
    "\n",
    "        input_embed = model.base_model.transformer.wte(input_ids)\n",
    "        input_with_prefix = torch.cat((prefix, input_embed), dim=1)\n",
    "\n",
    "        prefix_mask = torch.ones((1, prefix_len), dtype=torch.long)\n",
    "        full_attention_mask = torch.cat((prefix_mask, attention_mask), dim=1)\n",
    "\n",
    "        # Generate\n",
    "        output = model.base_model.generate(\n",
    "            inputs_embeds=input_with_prefix,\n",
    "            attention_mask=full_attention_mask,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_length=max_gen_length,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        pred = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        predictions.append(pred)\n",
    "        references.append([reference])  \n",
    "\n",
    "    bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "    rouge_result = rouge.compute(predictions=predictions, references=[r[0] for r in references])\n",
    "\n",
    "    # Display results\n",
    "    df = pd.DataFrame([{\n",
    "        \"BLEU\": round(bleu_result[\"bleu\"], 4),\n",
    "        \"ROUGE-1\": round(rouge_result[\"rouge1\"], 4),\n",
    "        \"ROUGE-2\": round(rouge_result[\"rouge2\"], 4),\n",
    "        \"ROUGE-L\": round(rouge_result[\"rougeL\"], 4)\n",
    "    }])\n",
    "    display(df)\n",
    "\n",
    "    return predictions, references\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761516a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BLEU</th>\n",
       "      <th>ROUGE-1</th>\n",
       "      <th>ROUGE-2</th>\n",
       "      <th>ROUGE-L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.1448</td>\n",
       "      <td>0.0618</td>\n",
       "      <td>0.1294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BLEU  ROUGE-1  ROUGE-2  ROUGE-L\n",
       "0  0.0142   0.1448   0.0618   0.1294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(\"src/smiles_sentiment_dataset_explicit.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "preds, refs = evaluate_model(model, tokenizer, test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
